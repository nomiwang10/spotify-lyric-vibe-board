# backend/api/ai_text.py

import os
import json
from typing import List, Optional
from fastapi import APIRouter, HTTPException
# ðŸ’¡ We need BaseModel for all request/response models
from pydantic import BaseModel, Field
from openai import OpenAI
from app.routes.ai_image import _build_prompt, _generate_image_data_url # ðŸ’¡ NEW: Import image helpers
from app.routes.ai_image import GenerateImageRequest
from app.routes.ai_client import get_openai_client

router = APIRouter(prefix="/api")


# --- Pydantic Models for REQUEST (Input from Frontend) ---

class LyricInputLine(BaseModel):
    # This is what the frontend's parseGeniusLyrics sends.
    id: int # The unique line index (0, 1, 2, ...)
    timestamp_ms: int 
    text: str

class LyricBatch(BaseModel):
    lines: List[LyricInputLine]
    targetLanguage: str = Field(default="English", alias="targetLanguage")


# --- Pydantic Models for RESPONSE (Output to Frontend) ---

class TranslatedLine(BaseModel):
    # The frontend is expecting an ID to link it back to the original lyric line.
    # CRITICAL: We changed the field name from 'time' to 'id' to match the input ID.
    id: int = Field(..., description="The unique ID (index) of the original lyric line.")
    text: str = Field(..., description="The translated text for the line.")

class VibeAnalysisResponse(BaseModel):
    # This entire model defines the *EXACT* structure the frontend expects 
    # (which it currently calls `translationData`).
    
    # ðŸ’¡ Changed to match the frontend state variable name `translatedLines`
    translatedLines: List[TranslatedLine] 
    
    imageUrl: str = Field(..., description="The base64 encoded image URL generated by DALL-E.")
    
    # ðŸ’¡ Changed to match the frontend state variable name `emotion`
    emotion: str = Field(..., description="The dominant emotion of the song (e.g., 'Nostalgia', 'Melancholy').")
    
    # ðŸ’¡ Changed to match the frontend state variable name `themes`
    themes: List[str] = Field(..., description="A list of 2-3 key themes of the song.")
    
    # ðŸ’¡ Added the image prompt field
    imagePrompt: str = Field(..., description="A detailed, visual prompt for an image generation model.")
    
    # Optional field for colors (if we keep it)
    colors: Optional[List[str]] = Field(None, description="2 hex color codes that represent the mood.")


@router.post(
    "/analyze-lyrics",
    # ðŸ’¡ CRITICAL: Use response_model to ensure FastAPI validates the outgoing data
    response_model=VibeAnalysisResponse 
)
def get_full_vibe_analysis(request: LyricBatch):
    """
    1. Calls GPT-4o for Vibe/Translation/Prompt.
    2. Calls DALL-E for the image.
    3. Returns all data.
    """
    try:
        # --- 1. Call GPT-4o for Vibe Analysis & Image Prompt (Logic from your file) ---
        # (This section remains largely the same, but now uses GPT-4o)

        # ... (Construct system_prompt and user_prompt) ...
        
        client = get_openai_client()
        
        response = client.chat.completions.create(
            model="gpt-4o", 
            # ... (messages and response_format) ...
        )
        
        ai_content = json.loads(response.choices[0].message.content)
        vibe_data = VibeAnalysisResponse(**ai_content)
        
        # --- 2. Call DALL-E for Image Generation ---
        
        # We need the full prompt that *would* have been sent to DALL-E
        # Reuse the payload structure from ai_image.py to build the prompt
        image_payload = GenerateImageRequest(
            lyric_lines=[line.text for line in request.lines], # Full list of original lines
            emotion=vibe_data.emotion,
            themes=vibe_data.themes
            # Note: The *final* image prompt is generated by GPT-4o in the prompt, 
            # but ai_image.py's _build_prompt wraps it in boilerplate. 
            # For simplicity, we'll use the one generated by GPT-4o if possible.
        )
        
        # Use the image prompt generated by GPT-4o directly (vibe_data.imagePrompt)
        # Or, pass the vibe data to the original _build_prompt helper:
        
        # Option A: Use the existing helper to build a collage prompt (Safer)
        dalle_prompt = _build_prompt(image_payload) 
        
        # Option B: Use the prompt generated by the analysis model (More direct)
        # dalle_prompt = vibe_data.imagePrompt 
        
        data_url = _generate_image_data_url(dalle_prompt)
        
        # --- 3. Return Combined Data ---
        
        # Since we generated the image, we now need to return the URL with the vibe data.
        # **Note:** You'll need to update your VibeAnalysisResponse model to include `imageUrl`.
        
        final_response = vibe_data.dict() # Convert Pydantic model to dict
        final_response["imageUrl"] = data_url # Add the image data URL
        
        return final_response

    except Exception as e:
        # This will catch both API errors and Pydantic validation errors
        print(f"Error in analyze_lyrics: {e}")
        raise HTTPException(status_code=500, detail=f"AI processing failed: {e}")
    
    